{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27076e55-2d63-430e-8cf7-364631db6d13",
   "metadata": {},
   "source": [
    "# **4 leptons analysis**   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dcb5d1-ee6b-4e35-ba5a-dc716d07fe2a",
   "metadata": {},
   "source": [
    "This notebook presents an analysis of simulated high-energy collision data targeting four-lepton final states at the Future Circular Collider - electron-positron mode (FCC-ee), focusing on Higgs boson decays into two Z bosons (one on-shell and one off-shell), which subsequently decay into four leptons, most often four muons. The four-lepton final state is often referred to as the \"golden channel\" due to its low background contamination and excellent reconstruction potential in detectors. \n",
    "\n",
    "From a physics standpoint, this process allows precision measurements of the Higgs properties, such as mass, couplings, and branching fractions, and serves as a probe for possible deviations from the Standard Model. However, the signal competes with several irreducible and reducible background processes: Standard Model production of ZZ pairs, WW pairs, Zqq, and other Higgs decay channels (H->bb, cc,ττ,WW,gg,Za) that may mimic the four-lepton final state."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d27d15d3-1b17-4eba-b330-4825e083e7a9",
   "metadata": {},
   "source": [
    "<img src=\"images_for_documented/signal.jpg\" style=\"height:300px; width:auto;\"> <img src=\"images_for_documented/backgrounds.jpg\" style=\"height:300px; width:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce85f45e-de4e-44ff-b94e-58685943610b",
   "metadata": {},
   "source": [
    "The notebook uses a modular analysis pipeline based on the Coffea framework in combination with Awkward Arrays and Dask, enabling scalable data handling and histogramming. The workflow begins with the definition of datasets corresponding to both signal and background samples, including their cross sections and integrated luminosities. A set of physics observables is then reconstructed, such as invariant masses of lepton pairs, four-lepton invariant mass, visible energy, missing momentum, and isolation variables. These are analyzed across successive selection stages (cutflows) designed to enhance the signal-to-background ratio.\n",
    "\n",
    "The analysis produces histograms and summary tables of event yields for each process, comparing simulated predictions under realistic detector assumptions. The results illustrate the reconstruction of the Higgs boson peak in the four-lepton channel and provide a framework to evaluate the performance of event selections, backgrounds, and overall sensitivity at the FCC-ee."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cc0cc8-2aa1-4034-9fc2-a65cc67a8ab6",
   "metadata": {},
   "source": [
    "Import all libraries and helper functions:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bdbab4a-9173-4740-a134-615b60c4623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install  git+https://github.com/davidlange6/coffea.git \n",
    "# !pip install coffea==2025.7.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d2441e-ddf3-493c-ab3f-069c7c1efc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import copy\n",
    "import glob\n",
    "import importlib\n",
    "\n",
    "import yaml\n",
    "import dask\n",
    "import hist\n",
    "import vector\n",
    "\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask_awkward as dak\n",
    "import hist.dask as hda\n",
    "import mplhep as hep\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "from numbers import Number\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "from dask.diagnostics import ProgressBar\n",
    "from dask.distributed import Client\n",
    "from coffea import processor, util\n",
    "from coffea.analysis_tools import PackedSelection, Cutflow\n",
    "from coffea.dataset_tools import apply_to_fileset, max_chunks, preprocess\n",
    "from coffea.nanoevents import BaseSchema, FCC\n",
    "from coffea.util import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "763af53a-e041-4ae3-86ae-4e5212ed1177",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector.register_awkward()\n",
    "pgb = ProgressBar()\n",
    "pgb.register()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0b87be-ada5-44f2-9402-fa625f149009",
   "metadata": {},
   "source": [
    "# **Config**     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5c3fe3-b345-4066-b510-f2c186962024",
   "metadata": {},
   "source": [
    "Define the process configuration dictionary: it stores the collider, campaign, detector setup, integrated luminosity, process, event generator, energy, and a list of signal and background datasets with fractions and cross sections to be analyzed. The fractions (1 * reduction_factor) of events to use from each dataset - downscale the number of events processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bd93fd2-f669-494f-968c-633098554b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_factor = 0.000000001\n",
    "ecm = 240.0 ## collision energy \\sqrt(s) in GeV\n",
    "process = {\n",
    "    'collider': 'FCCee',\n",
    "    'campaign': 'winter2023',\n",
    "    'detector': 'IDEA',\n",
    "    'intLumi': 10.80e+06, ## integrated luminosity in pb-1\n",
    "    'ana_tex': 'e^{+}e^{-} \\\\rightarrow ZH \\\\rightarrow 4\\\\mu+ X',\n",
    "    'delphesVersion': '3.4.2',\n",
    "    'energy': ecm, #in GeV\n",
    "    'samples': {\n",
    "        # Signal\n",
    "        'wzp6_ee_qqH_HZZ_llll_ecm240': {'fraction': 1 * reduction_factor, 'cross_section': 0.00003777},\n",
    "        'wzp6_ee_nunuH_HZZ_ecm240': {'fraction': 1 * reduction_factor, 'cross_section': 0.00122},\n",
    "        # Backgrounds\n",
    "        'p8_ee_Zqq_ecm240': {'fraction': 1 * reduction_factor, 'cross_section': 52.6539},\n",
    "        'p8_ee_ZZ_ecm240': {'fraction': 1 * reduction_factor, 'cross_section': 1.35899},\n",
    "        'p8_ee_WW_ecm240': {'fraction': 1 * reduction_factor, 'cross_section': 16.4385},\n",
    "        'wzp6_ee_tautauH_HWW_ecm240': {'fraction': 1 * reduction_factor, 'cross_section': 0.001453},\n",
    "        'wzp6_ee_ccH_HWW_ecm240': {'fraction': 1 * reduction_factor, 'cross_section': 0.005023},\n",
    "        'wzp6_ee_bbH_HWW_ecm240': {'fraction': 1 * reduction_factor, 'cross_section': 0.00645},\n",
    "        'wzp6_ee_mumuH_HWW_ecm240': {'fraction': 1 * reduction_factor, 'cross_section': 0.001456},\n",
    "        'wzp6_ee_mumuH_Hcc_ecm240': {'fraction': 1 * reduction_factor, 'cross_section': 0.0001956},\n",
    "        'wzp6_ee_mumuH_Hbb_ecm240': {'fraction': 1 * reduction_factor, 'cross_section': 0.00394},\n",
    "        'wzp6_ee_mumuH_Hgg_ecm240': {'fraction': 1 * reduction_factor, 'cross_section': 0.0005538},\n",
    "        'wzp6_ee_mumuH_HZa_ecm240': {'fraction': 1 * reduction_factor, 'cross_section': 0.00001037}\n",
    "    }\n",
    "}\n",
    "fractions = {name: info['fraction'] for name, info in process['samples'].items()}\n",
    "cross_sections = {name: info['cross_section'] for name, info in process['samples'].items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffc3ba8-1788-4fb4-b1db-d175cd35ff14",
   "metadata": {},
   "source": [
    "Set up the configuration parameters for the analysis run.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18af1fee-0abf-4fd7-bc32-16777aa332c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_yaml_dict = \"../../filesets/\" ## path to yaml files describing datasets\n",
    "output_path = \"Batch\" ## directory to save results\n",
    "use_schema = \"FCC\" ## which data schema to use\n",
    "schema_version = \"pre-edm4hep1\" ## which data schema version to use\n",
    "## chunks for dataset processing control\n",
    "runner_maxchunks = 1000\n",
    "runner_chunks = 1\n",
    "runner_cachedchunks = True\n",
    "## name the output\n",
    "output_filename = \"4leptons\"\n",
    "output_file = output_filename+\".coffea\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d859576b-38f1-42e7-b66f-c57746eb0f98",
   "metadata": {},
   "source": [
    "Define the set of plots to be generated and their properties.   \n",
    "Convert this dictionary into a pandas DataFrame for easier access when plotting.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a2ba493-7415-4127-9647-3419df34d1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = {\n",
    "    'selectedmuons_p':{'name':'selected_muons_p','title':'$\\\\mu_p$ [GeV]','xlabel':'$p_T$ [GeV]','ylabel':'Events','bins':250,'xmin':0,'xmax':250},\n",
    "\n",
    "    'fourmuons_mass':{'name':'fourMuons_mass','title':'$M_{4\\\\mu}$ [GeV]','xlabel':'$Mass$ [GeV]','ylabel':'Events','bins':50,'xmin':0,'xmax':250},\n",
    "    'fourmuons_pmin':{'name':'fourMuons_pmin','title':'$(P_{4\\\\mu})_{min}$ [GeV]','xlabel':'$p_{min}$ [GeV]','ylabel':'Events','bins':20,'xmin':0,'xmax':100},\n",
    "\n",
    "    'Z_res_mass':{'name':'zll_mass','title':'On-shell $M_{\\\\mu\\\\mu}$ [GeV]','xlabel':'$Mass$ [GeV]','ylabel':'Events','bins':50,'xmin':0,'xmax':250},\n",
    "    'Z_non_res_mass':{'name':'non_res_Z_m','title':'Off-shell $M_{\\\\mu\\\\mu}$ [GeV]','xlabel':'$Mass$ [GeV]','ylabel':'Events','bins':50,'xmin':0,'xmax':250},\n",
    "\n",
    "    'vis_e_woMuons':{'name':'vis_e_other_particles','title':'Visible Energy excluding muons [GeV]','xlabel':'$E$ [GeV]','ylabel':'Events','bins':50,'xmin':0,'xmax':250},\n",
    "    'iso_least_isolated_muon':{'name':'fourMuons_min_iso','title':'iso(least isolated muon)','xlabel':'iso','ylabel':'Events','bins':50,'xmin':0,'xmax':20},\n",
    "    'missing_p':{'name':'pmiss','title':'missing p [GeV]','xlabel':'$p^{miss}$ [GeV]','ylabel':'Events','bins':50,'xmin':0,'xmax':250},\n",
    "    'cos_theta_miss':{'name':'cosTheta_miss','title':'Cos(Theta_miss)','xlabel':'$cos_{miss}\\\\theta$','ylabel':'Events','bins':100,'xmin':0,'xmax':1},\n",
    "}\n",
    "plot_props = pd.DataFrame(plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0872855-857b-4e81-8701-fa2dc453ad7c",
   "metadata": {},
   "source": [
    "Plotting variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f476f74-feec-4be4-9e19-7077114f2d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current configuration:\n",
      "\tinput_path:\tBatch/\n",
      "\toutput_file:\t4leptons.coffea\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stack = [True, False] ## stacked/unstacked\n",
    "log = [True, False] ## linear/log scale\n",
    "formats = ['png']\n",
    "req_plots = ['selectedmuons_p','fourmuons_mass','fourmuons_pmin','Z_res_mass','Z_non_res_mass','vis_e_woMuons','iso_least_isolated_muon','missing_p','cos_theta_miss']\n",
    "req_hists = {\n",
    "        #Signal\n",
    "        \"qqH_HZZ\":{\"type\":'Signal',\"datasets\":['wzp6_ee_qqH_HZZ_llll_ecm240'],\"color\":'red'},\n",
    "        \"nunuH_HZZ\":{\"type\":'Signal',\"datasets\":['wzp6_ee_nunuH_HZZ_ecm240'],\"color\":'orange'},\n",
    "        #Background\n",
    "        \"ZZ\":{\"type\":'Background',\"datasets\":['p8_ee_ZZ_ecm240'],\"color\":'blue'},\n",
    "        \"Zqq\":{\"type\":'Background',\"datasets\":['p8_ee_Zqq_ecm240'],\"color\":'yellow'},\n",
    "        \"mumuH_Hjj\":{\"type\":'Background',\"datasets\":['wzp6_ee_mumuH_Hbb_ecm240','wzp6_ee_mumuH_Hcc_ecm240','wzp6_ee_mumuH_Hgg_ecm240',],\"color\":'cyan'},\n",
    "        \"WW\":{\"type\":'Background',\"datasets\":['p8_ee_WW_ecm240'],\"color\":'gray'},\n",
    "        \"HWW\":{\"type\":'Background',\"datasets\":['wzp6_ee_mumuH_HWW_ecm240','wzp6_ee_bbH_HWW_ecm240','wzp6_ee_tautauH_HWW_ecm240','wzp6_ee_ccH_HWW_ecm240'],\"color\":'teal'},\n",
    "        \"mumuH_HZa\":{\"type\":'Background',\"datasets\":['wzp6_ee_mumuH_HZa_ecm240'],\"color\":'green'},\n",
    "\n",
    "}\n",
    "input_path     = output_path\n",
    "plot_path      = 'outputs/plots/'\n",
    "\n",
    "## input data location\n",
    "plotter_input=input_path\n",
    "input_path = plotter_input+\"/\"\n",
    "print(f'Current configuration:\\n\\tinput_path:\\t{input_path}\\n\\toutput_file:\\t{output_file}\\n')\n",
    "\n",
    "# Extra cosmetics\n",
    "yield_table_scale = 0.7 # reduce font sizes in yield table by this factor, so that they fit in the figure area\n",
    "legend_location = (0.64, 0.64) # Coordinate of the bottom left corner of the legend wrt to the plot area\n",
    "FCC_text_location = (0.30,1.02) # Coordinate of the bottom left corner of the FCC text wrt to the plot area\n",
    "custom_mc_order = ['ZZ', 'Zqq', 'WW', 'mumuH_Hjj', 'HWW', 'mumuH_HZa'] # From top to bottom\n",
    "Reverse_legend_labels = False # Reverse legend order without changing the stack order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de724b47-9843-4a19-b219-73d39450b165",
   "metadata": {},
   "source": [
    "# **Functions**   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fa9728-1af3-4ccb-8ae2-28f6859d568c",
   "metadata": {},
   "source": [
    "**resonanceBuilder_mass** builds dilepton candidates and selects oppositely charged pairs whose invariant mass is closest to a given resonance mass.   \n",
    "**Input**: resonance_mass (float, target mass), leptons (awkward array with momentum, charge, and index), and optional MC-related arguments.   \n",
    "**Output**: an awkward array of dilepton candidates (with summed four-momentum, charge, and original lepton indices), sorted by closeness to the target resonance mass.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a70c4aa3-b0b5-4f60-8bd9-f72b1b4e27ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resonanceBuilder_mass(resonance_mass=None, use_MC_Kinematics=False, leptons=None, MCRecoAssociations=None, ReconstructedParticles=None, MCParticles=None):\n",
    "    if leptons is None:\n",
    "        raise AttributeError(\"No leptons passed\")\n",
    "    # Create all the combinations\n",
    "    combs = ak.combinations(leptons,2)\n",
    "    # Get dileptons\n",
    "    lep1 , lep2 = ak.unzip(combs)\n",
    "    di_lep = lep1 + lep2 # This process drops any other field except 4 momentum fields\n",
    "\n",
    "    di_lep[\"charge\"] =  lep1.charge + lep2.charge\n",
    "    di_lep[\"l1_index\"] = lep1.index\n",
    "    di_lep[\"l2_index\"] = lep2.index\n",
    "\n",
    "    # Choose oppositely charged leptons\n",
    "    di_lep = di_lep[di_lep.charge == 0]\n",
    "\n",
    "    # Sort by closest mass to the resonance value\n",
    "    sort_mask = ak.argsort(abs(resonance_mass-di_lep.mass), axis=1)\n",
    "    Reso = di_lep[sort_mask]\n",
    "\n",
    "    return Reso\n",
    "    return Reso, used_lep1, used_lep2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe5a6ad-cb80-49eb-9370-30f413ccef04",
   "metadata": {},
   "source": [
    "**getTwoHighestPMuons** selects the two highest-momentum muons in each event, requiring them to have opposite charge.   \n",
    "**Input**: muons (awkward array with momentum p and charge).   \n",
    "**Output**: (muon1, muon2, mask) where muon1 and muon2 are the leading opposite-charge muons (or None if not found), and mask is a boolean per event indicating whether a valid opposite-charge pair exists.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b4e0646-da0a-4cf0-a420-1ef45dfcc5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTwoHighestPMuons(muons):\n",
    "    # if not ak.all(ak.num(muons, axis=1) > 1 ):\n",
    "    # raise IndexError(\"Need at least two particles!\")\n",
    "    sorted_muons_p = ak.argsort(muons.p, ascending=False)\n",
    "    sorted_muons = muons[sorted_muons_p]\n",
    "\n",
    "    # First particle is always selected, if the second one has the opposite charge, then its accepted otherwise we move on to the third and so on\n",
    "    # Interestingly, this type of operation is non trivial in an array format\n",
    "    first_muon, other_muons = sorted_muons[:, 0:1], sorted_muons[:, 1:]\n",
    "\n",
    "    # prepare before cartesian : replace none with []\n",
    "    first_muon = ak.fill_none(first_muon, [], axis=0)\n",
    "    other_muons = ak.fill_none(other_muons, [], axis=0)\n",
    "    # All combinations\n",
    "    all_comb = ak.cartesian([first_muon, other_muons])\n",
    "    l1, l2 = ak.unzip(all_comb)\n",
    "    charge_mask = l1.charge!= l2.charge\n",
    "    l1 = l1[charge_mask]\n",
    "    l2 = l2[charge_mask]\n",
    "\n",
    "    at_least_one_opp_charged = ak.sum(charge_mask, axis=1) > 0 # is an event selection\n",
    "    \n",
    "    return ak.firsts(l1), ak.firsts(l2), at_least_one_opp_charged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c7daba-4191-45ad-89c1-21c21f9dbfb2",
   "metadata": {},
   "source": [
    "**sum_all** computes the total four-momentum of a collection of particles.   \n",
    "**Input**: array_of_lv (awkward array of Lorentz vectors with fields px, py, pz, E).   \n",
    "**Output**: an awkward array with summed four-momentum components, returned as a Momentum4D object.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c2e92cc-449f-4b37-b7d2-f215c974826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_all(array_of_lv):\n",
    "    out = ak.zip(\n",
    "        {\n",
    "            \"px\":ak.sum(array_of_lv.px , axis=1),\n",
    "            \"py\":ak.sum(array_of_lv.py , axis=1),\n",
    "            \"pz\":ak.sum(array_of_lv.pz , axis=1),\n",
    "            \"E\":ak.sum(array_of_lv.E , axis=1)\n",
    "        },\n",
    "        with_name=\"Momemtum4D\"\n",
    "    )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c48c6d6-68e9-433d-9ed9-f75155d612b3",
   "metadata": {},
   "source": [
    "**coneIsolation** computes the isolation of a particle by summing momenta of nearby charged and neutral particles within a ΔR cone.   \n",
    "**Input**: particle (awkward array with momentum p and four-vector info), rest_of_the_particles (awkward array with charge and momentum), min_dr (float, default=0.0), max_dr (float, default=0.4).   \n",
    "**Output**: isolation ratio = (sum of nearby particle momenta) / (particle momentum).   \n",
    "**Refer**: https://github.com/delphes/delphes/blob/master/modules/Isolation.cc#L154   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41682a53-3a5e-4a2b-bc58-28620bc34673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coneIsolation(particle, rest_of_the_particles, min_dr=0.0 , max_dr=0.4):\n",
    "    neutral_particles = ak.mask(rest_of_the_particles, rest_of_the_particles.charge == 0)\n",
    "    charged_particles = ak.mask(rest_of_the_particles, rest_of_the_particles.charge != 0)\n",
    "\n",
    "    n_combs = ak.cartesian((particle,neutral_particles[:,np.newaxis]), axis=1)\n",
    "    n1,n2 = ak.unzip(n_combs)\n",
    "    c_combs = ak.cartesian((particle,charged_particles[:,np.newaxis]), axis=1)\n",
    "    c1,c2 = ak.unzip(c_combs)\n",
    "\n",
    "    n_angle = n1.deltaangle(n2)\n",
    "    c_angle = c1.deltaangle(c2)\n",
    "\n",
    "    n_angle_mask = (n_angle < max_dr) & (n_angle >= min_dr)\n",
    "    c_angle_mask = (c_angle < max_dr) & (c_angle >= min_dr)\n",
    "\n",
    "    filtered_neutral = n2[n_angle_mask]\n",
    "    filtered_charged = c2[c_angle_mask]\n",
    "\n",
    "    sumNeutral = ak.sum(filtered_neutral.p, axis=2)\n",
    "    sumCharged = ak.sum(filtered_charged.p, axis=2)\n",
    "\n",
    "    total_sum = sumNeutral + sumCharged\n",
    "\n",
    "    ratio = total_sum / particle.p\n",
    "\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76378e49-9c78-4501-9968-986f4c5481c5",
   "metadata": {},
   "source": [
    "**recoilBuilder** computes the recoil four-momentum against a given system.   \n",
    "**Input**: vec (awkward array with four-vector fields px, py, pz, E), ecm (float, center-of-mass energy).   \n",
    "**Output**: an awkward array Momentum4D representing the recoil vector (-vec in momentum, ecm - E in energy).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1caec290-a72c-48fb-bd6a-ebd67edf1f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recoilBuilder(vec, ecm):\n",
    "    Recoil = ak.zip({\"px\":0.0-vec.px,\"py\":0.0-vec.py,\"pz\":0.0-vec.pz,\"E\":ecm-vec.E},with_name=\"Momentum4D\")\n",
    "    return Recoil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520f5936-5a95-404a-be43-b89aba2018c8",
   "metadata": {},
   "source": [
    "**remove** removes elements from an array based on specified indices.   \n",
    "**Input**: array (awkward array), idx (awkward array containing indices of elements to remove).   \n",
    "**Output**: a filtered awkward array with the specified elements removed.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a2327d3-d5c3-40e0-b0d0-954afd06af2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove(array, idx):\n",
    "    index = idx.index\n",
    "    all_index = ak.local_index(array,axis=1)\n",
    "    i,a = ak.unzip(ak.cartesian([index[:,np.newaxis] ,all_index], nested=True))\n",
    "    c = a == i\n",
    "    d = ak.firsts(c)\n",
    "    s = ak.sum(d, axis=2)\n",
    "    kl = s == 1\n",
    "    return array[~kl]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d5d53f-6bb0-49e1-9000-7aba51163c3e",
   "metadata": {},
   "source": [
    "**get_1Dhist** creates a 1D histogram for a given variable.   \n",
    "**Input**: name (string, used to fetch histogram properties from plot_props), var (array-like data to fill), flatten (bool, default=False; if True, flattens nested arrays).   \n",
    "**Output**: a filled 1D histogram object (hda.Hist).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1478e0d5-c24a-45bc-8a9e-75fd780dcec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_1Dhist(name, var, flatten=False):\n",
    "    props = plot_props[name]\n",
    "    if flatten:\n",
    "        var = ak.flatten(var, axis=None)\n",
    "    var = var[~ak.is_none(var, axis=0)]\n",
    "\n",
    "    if isinstance(var, ak.Array):\n",
    "        var = ak.to_numpy(var)\n",
    "\n",
    "    hist = hda.Hist.new.Reg(props.bins, props.xmin, props.xmax).Double().fill(var)\n",
    "    hist = hist.to_numpy()\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b086d2-fa63-4276-ba36-2fa74c64ea6a",
   "metadata": {},
   "source": [
    "**create_mask** creates a boolean mask where elements of a and b are not equal to c.   \n",
    "**Input**: a, b, c (arrays or values of the same shape).   \n",
    "**Output**: boolean array where True indicates both a and b differ from c.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b007667-f543-45ee-b2cf-255fb33d29ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(a, b, c):\n",
    "    mask1 = a != c\n",
    "    mask2 = b != c\n",
    "    mask = mask1 & mask2\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742e2841-77c8-4d7e-8162-29168e336c8e",
   "metadata": {},
   "source": [
    "**get_schema** imports and returns a particle physics event schema.   \n",
    "**Input**: use_schema (str, \"BaseSchema\" or \"FCC\", default=\"BaseSchema\"), schema_version (str, default=\"latest\", only used for FCC).   \n",
    "**Output**: tuple (schema_handler, schema, schema_name) containing the handler class, the schema object, and a descriptive name.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "940609d6-5e3a-41db-84a5-c6467202fe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema(use_schema: str = \"BaseSchema\", schema_version: str = \"latest\"):\n",
    "    if use_schema == \"BaseSchema\":\n",
    "        module_name = \"coffea.nanoevents\"\n",
    "        schema_caller = \"BaseSchema\"\n",
    "    elif use_schema == \"FCC\":\n",
    "        module_name = \"coffea.nanoevents\"\n",
    "        schema_caller = \"FCC\"\n",
    "    else:\n",
    "        raise ValueError(f\"The requested schema '{use_schema}' is not available.\")\n",
    "\n",
    "    module = importlib.import_module(module_name)\n",
    "    schema_handler = getattr(module, schema_caller, None)\n",
    "\n",
    "    if schema_handler is None:\n",
    "        raise ImportError(f\"Could not import schema handler '{schema_caller}' from '{module_name}'.\")\n",
    "\n",
    "    if use_schema == \"FCC\":\n",
    "        schema = schema_handler.get_schema(schema_version)\n",
    "        schema_name = f\"FCC.get_schema('{schema_version}')\"\n",
    "        if schema is None:\n",
    "            raise ValueError(f\"The requested version '{schema_version}' for schema '{use_schema}' is not available.\")\n",
    "    else:\n",
    "        schema = schema_handler\n",
    "        schema_name = \"BaseSchema\"\n",
    "\n",
    "    return schema_handler, schema, schema_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1b926a-5af2-46a5-b7a1-d87f36bb0f47",
   "metadata": {},
   "source": [
    "**load_yaml_fileinfo** loads yaml configuration files for a given process’s filesets.   \n",
    "**Input**: process (dict with keys collider, campaign, detector, and samples).   \n",
    "**Output**: dictionary mapping each sample name to its loaded yaml content.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f810722-2010-44ce-b7b6-4f5a2b89ac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yaml_fileinfo(process):\n",
    "    onlinesystem_path = '/cvmfs/fcc.cern.ch'\n",
    "    # localsystem_path = './../../../../../filesets/'\n",
    "    if 'local_yaml_dict' in globals():\n",
    "        localsystem_path = local_yaml_dict\n",
    "    else :\n",
    "        localsystem_path = 'filesets/'\n",
    "    path = '/'.join(\n",
    "        [\n",
    "         'FCCDicts',\n",
    "         'yaml',\n",
    "         process['collider'],\n",
    "         process['campaign'],\n",
    "         process['detector']\n",
    "        ])\n",
    "    if os.path.exists(onlinesystem_path):\n",
    "        print(f'Connected to {onlinesystem_path}')\n",
    "        filesystem_path = onlinesystem_path\n",
    "    else:\n",
    "        print(onlinesystem_path+' is not available.\\nTrying to find local copies of the yaml files ...')\n",
    "        filesystem_path = localsystem_path\n",
    "    yaml_dict = {}\n",
    "    for sample in process['samples']:\n",
    "        full_path = '/'.join([filesystem_path, path, sample, 'merge.yaml'])\n",
    "        try :\n",
    "            with open(full_path) as f:\n",
    "                dict = yaml.safe_load(f)\n",
    "            print('Loaded : '+full_path)\n",
    "        except:\n",
    "            raise FileNotFoundError(f'Could not find yaml files at {filesystem_path}')\n",
    "        yaml_dict[sample] = dict\n",
    "    return yaml_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00ff66d-6747-455d-b292-69e27063a869",
   "metadata": {},
   "source": [
    "**assign_events** finds the index in a cumulative sum array corresponding to a target number of events.   \n",
    "**Input**: cumsum (1D array of cumulative sums), needed_events (int or float, target number of events).   \n",
    "**Output**: index in cumsum where the cumulative sum first reaches or exceeds needed_events.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d55c72b-b026-4b36-9b22-dc617241136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_events(cumsum, needed_events):\n",
    "    if needed_events > cumsum[-1]:\n",
    "        return len(cumsum)-1\n",
    "    diff = cumsum - needed_events\n",
    "    return np.argwhere(diff >= 0)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab53739-4032-43b3-b1ed-af95b6e4d1cd",
   "metadata": {},
   "source": [
    "**get_fileset** selects a fraction of files from a yaml-defined dataset and returns them in a Dask-compatible format.   \n",
    "**Input**: yaml_dict (dict of dataset metadata), fraction (dict mapping process names to fraction of events to select), skipbadfiles (bool, default=True), redirector (str, optional path prefix).   \n",
    "**Output**: dictionary mapping each process key to its selected files and associated 'events'.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eaeffe39-4098-4e83-8985-98b04c2caf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fileset(yaml_dict, fractions, skipbadfiles=True, redirector=''):\n",
    "    output_fileset_dictionary = {}\n",
    "    print('_________Loading fileset__________')\n",
    "    for key in yaml_dict.keys():\n",
    "        output_fileset_dictionary[key] = {}\n",
    "        # nbad = yaml_dict[key]['merge']['nbad']\n",
    "        # ndone = yaml_dict[key]['merge']['ndone']\n",
    "        nevents = yaml_dict[key]['merge']['nevents']\n",
    "        outdir = yaml_dict[key]['merge']['outdir']\n",
    "        outfiles = yaml_dict[key]['merge']['outfiles']\n",
    "        outfilesbad = yaml_dict[key]['merge']['outfilesbad']\n",
    "        proc = yaml_dict[key]['merge']['process']\n",
    "        # size = yaml_dict[key]['merge']['size']\n",
    "        # sumofweights = yaml_dict[key]['merge']['sumofweights']\n",
    "        out = np.array(outfiles)\n",
    "        bad = np.array(outfilesbad)\n",
    "\n",
    "        # Remove bad files\n",
    "        if (bad.size != 0) & skipbadfiles :\n",
    "            filenames_bad = bad[:,0]\n",
    "            y = out\n",
    "            for row in range(out.shape[0]) :\n",
    "                file = out[row,0]\n",
    "                if file in filenames_bad:\n",
    "                    y = np.delete(y , (row), axis=0)\n",
    "            out = y\n",
    "\n",
    "        filenames = out[:,0]\n",
    "        file_events = out[:,1].astype('int32')\n",
    "        cumulative_events = np.cumsum(file_events)\n",
    "\n",
    "        frac = fractions[proc]\n",
    "        needed_events = frac*nevents\n",
    "\n",
    "        # get closest value and index to the needed events\n",
    "        # index = np.abs(cumulative_events - needed_events).argmin()\n",
    "        #\n",
    "        # FCCAnalyses uses the ceiling value instead of closest value\n",
    "        # Eg. if we need 341 events and cumsum is [...300 , 400 ...] , then 400 events are choosen, even though 341 events is more closer to 300 events in comparison to 400\n",
    "        # To get this ceiling value, we could get the most positive value from the cumulative_events - needed_events difference\n",
    "        index = assign_events(cumulative_events, needed_events)\n",
    "\n",
    "        assigned_events = cumulative_events[index]\n",
    "        assigned_files = filenames[:index+1]\n",
    "\n",
    "        # Summary\n",
    "        print('----------------------------------')\n",
    "        print(f'----------{key}---------')\n",
    "        print('----------------------------------')\n",
    "        print(f'Total available events = {nevents}')\n",
    "        print(f'Fraction needed = {frac}')\n",
    "        print(f'Needed events = {needed_events}')\n",
    "        print(f'Assigned events = {assigned_events}')\n",
    "        print(f'Number of files = {len(assigned_files)}')\n",
    "        print('Files:')\n",
    "\n",
    "        # At the same time get the dictionary\n",
    "        fileset_by_key = {}\n",
    "        for file in assigned_files:\n",
    "            print(f'\\t {redirector+outdir+file}')\n",
    "            fileset_by_key[redirector+outdir+file] = 'events'\n",
    "        output_fileset_dictionary[key]['files'] = fileset_by_key\n",
    "    return output_fileset_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761674a5-0bd6-40ea-9e66-78d1370953de",
   "metadata": {},
   "source": [
    "**break_into_many** splits a fileset into n roughly equal smaller filesets.   \n",
    "**Input**: input_fileset (dict of files organized by dataset), n (int, number of splits).   \n",
    "**Output**: list of n filesets, each containing a subset of the original files.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4baaf19e-15a0-4f30-80e0-8a723594a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_into_many(input_fileset,n):\n",
    "    # Create an indexed fileset\n",
    "    fileset = copy.deepcopy(input_fileset)\n",
    "    index = 0\n",
    "    for dataset in input_fileset.keys():\n",
    "        for filename,treename in input_fileset[dataset]['files'].items():\n",
    "            fileset[dataset]['files'][filename] = {'treename': treename, 'index': index}\n",
    "            index += 1\n",
    "\n",
    "    # Split the array as required\n",
    "    nfiles = sum([len(fileset[dataset]['files']) for dataset in fileset.keys()])\n",
    "    if n == 0 :\n",
    "        return [input_fileset]\n",
    "    elif n > 0 and n <= index:\n",
    "        index_split = np.array_split(np.arange(nfiles),n)\n",
    "    else :\n",
    "        raise ValueError(f'Allowed values of n between 0 and {index}')\n",
    "\n",
    "    # Choose the required indices for each split\n",
    "    raw = [copy.deepcopy(input_fileset) for i in range(n)]\n",
    "    for f in range(n):\n",
    "        for dataset in fileset.keys():\n",
    "            for event in fileset[dataset]['files'].keys():\n",
    "                if not fileset[dataset]['files'][event]['index'] in index_split[f]:\n",
    "                    del raw[f][dataset]['files'][event]\n",
    "\n",
    "    #remove empty fields\n",
    "    out = copy.deepcopy(raw)\n",
    "    for f in range(n):\n",
    "        for dataset in raw[f].keys():\n",
    "            if len(raw[f][dataset]['files']) == 0 :\n",
    "                del out[f][dataset]\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e0e0bd-e7ed-48c9-84e8-e4855469fabb",
   "metadata": {},
   "source": [
    "**accumulate** recursively merges a list of dictionaries, supporting: numeric summation, list concatenation, set union, histogram addition (from `hist`), nested dictionaries, key exceptions (preserve first).   \n",
    "**Input**: dicts (list of dictionaries).   \n",
    "**Output**: single dictionary with merged values: numeric values summed, lists concatenated, sets unioned, histograms added, nested dictionaries merged, and specified exceptions preserved.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0614fcb-faa9-4ba9-9e4a-e32e8d75ee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate(dicts):\n",
    "    try:\n",
    "        from hist import Hist\n",
    "    except ImportError:\n",
    "        Hist = None  # Skip if hist is not available\n",
    "\n",
    "    exception_list = {'Labels'}\n",
    "    grouped = defaultdict(list)\n",
    "\n",
    "    for d in dicts:\n",
    "        for k, v in d.items():\n",
    "            grouped[k].append(v)\n",
    "\n",
    "    outdict = {}\n",
    "\n",
    "    for key, values in grouped.items():\n",
    "        first = values[0]\n",
    "\n",
    "        if key in exception_list:\n",
    "            outdict[key] = first\n",
    "        elif all(isinstance(v, dict) for v in values):\n",
    "            outdict[key] = accumulate(values)\n",
    "        elif all(isinstance(v, list) for v in values):\n",
    "            outdict[key] = sum(values, [])  # concatenate\n",
    "        elif all(isinstance(v, set) for v in values):\n",
    "            result = set()\n",
    "            for v in values:\n",
    "                result |= v\n",
    "            outdict[key] = result\n",
    "        elif Hist and all(isinstance(v, Hist) for v in values):\n",
    "            total = values[0]\n",
    "            for v in values[1:]:\n",
    "                total += v\n",
    "            outdict[key] = total\n",
    "        elif all(isinstance(v, Number) for v in values):\n",
    "            outdict[key] = sum(values)\n",
    "        else:\n",
    "             # Mixed types or strings — keep the first\n",
    "            outdict[key] = first\n",
    "\n",
    "    return outdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846812c3-ea47-4887-b953-be94340c636a",
   "metadata": {},
   "source": [
    "**get_xsec_scale** computes the scale factor for a dataset based on its cross section, number of generated events, and target luminosity.   \n",
    "**Input**: dataset (str, dataset name), raw_events (int, number of generated events), Luminosity (float, target integrated luminosity).   \n",
    "**Output**: scale factor (float).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36bb5615-10d7-4892-80a2-19bd5ff439f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xsec_scale(dataset, raw_events, Luminosity):\n",
    "    xsec = cross_sections[dataset] #in per picobarn\n",
    "    if raw_events > 0:\n",
    "        sf = (xsec*Luminosity)/raw_events\n",
    "    else :\n",
    "        raise ValueError('Raw events less than of equal to zero!')\n",
    "    return float(sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69ba388-1878-48b4-829a-55976a5d8568",
   "metadata": {},
   "source": [
    "**hist_sorter** reorders histograms, labels, and colors according to a preferred order for plotting.   \n",
    "**Input**: preferred_order (list of labels in desired order), unsorted_hists (list of histogram objects), unsorted_labels (list of labels), unsorted_colors (list of colors).   \n",
    "**Output**: tuple (sorted_hists, sorted_labels, sorted_colors) sorted according to preferred_order.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85ff8ce0-0751-43d8-a03c-a93933f9cfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_sorter(preferred_order, unsorted_hists, unsorted_labels, unsorted_colors):\n",
    "    sorted_hists, sorted_labels, sorted_colors = [], [], []\n",
    "    for l in preferred_order[::-1]:\n",
    "        pos = unsorted_labels.index(l)\n",
    "        sorted_hists.append(unsorted_hists[pos])\n",
    "        sorted_labels.append(unsorted_labels[pos])\n",
    "        sorted_colors.append(unsorted_colors[pos])\n",
    "    return sorted_hists, sorted_labels, sorted_colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5944e4-59c3-48d1-af5c-600f41631d8d",
   "metadata": {},
   "source": [
    "**yield_plot** creates a yield summary plot as a styled table for multiple datasets.   \n",
    "**Input**: name (str, output file base name), title (str, plot title), keys (list of dataset keys), scaled (list of scaled yield dictionaries), unscaled (list of unscaled yield dictionaries), formats (list of output file formats, e.g., ['png', 'pdf']), path (str, output directory), plot_width, plot_height (optional, figure size).   \n",
    "**Output**: none (saves plot files).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d662fd6-8aaa-4718-b936-b27b4aece4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_plot(name, title, keys, scaled, unscaled, formats, path, plot_width=8, plot_height=8):\n",
    "    fig, ax = plt.subplots(figsize=(plot_width,plot_height))\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.text(0.25, 1.02, 'FCC Analyses: FCC-ee Simulation (Delphes)', fontsize=10, horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n",
    "    ax.text(0.92, 1.02, '$\\\\sqrt{s} = '+str(process['energy'])+' GeV$', fontsize=10, horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n",
    "    ax.text(0.10, 0.95, process['collider'], fontsize=14, horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "    ax.text(0.10, 0.88,'Delphes Version: '+ process['delphesVersion'], fontsize=14, horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "    ax.text(0.10, 0.81, 'Signal : $'+process['ana_tex']+'$', fontsize=14, horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "    ax.text(0.10, 0.74, '$L = '+str(process['intLumi']/1e6)+' ab^{-1}$', fontsize=14, horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "    table_scale = globals().get(\"yield_table_scale\", 1)\n",
    "    print(\"table_scale\", table_scale)\n",
    "    level, linespacing = 0.72, 0.05\n",
    "    for scale,obs in zip(['UNSCALED','SCALED'],[unscaled,scaled]):\n",
    "        ax.text(0.02, level, scale, weight='bold', fontsize=int(13*table_scale), horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "        level -= linespacing*table_scale\n",
    "        ax.text(0.02, level, 'Sample', weight='bold', fontsize=int(12*table_scale), horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "        ax.text(0.30, level, 'Type', weight='bold', fontsize=int(12*table_scale), horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "        ax.text(0.49, level, 'Raw', weight='bold', fontsize=int(12*table_scale), horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "        ax.text(0.68, level, 'Yield', weight='bold', fontsize=int(12*table_scale), horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "        ax.text(0.87, level, 'Yield %', weight='bold', fontsize=int(12*table_scale), horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "        for i in range(len(keys)):\n",
    "            datasets = req_hists[list(keys)[i]]['datasets']\n",
    "            Type = req_hists[list(keys)[i]]['type']\n",
    "            color = req_hists[list(keys)[i]]['color']\n",
    "            yield_text = str(round(obs[i]['Cutflow'].values()[-1],2))\n",
    "            raw_text = str(round(obs[i]['Cutflow'].values()[0],2))\n",
    "            percentage = str(round(obs[i]['Cutflow'].values()[-1]*100/obs[i]['Cutflow'].values()[0],2))\n",
    "            level -= linespacing*table_scale\n",
    "            ax.text(0.02, level, list(keys)[i], fontsize=int(10*table_scale), color=color,horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "            ax.text(0.30, level, Type, color=color, fontsize=int(12*table_scale), horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "            ax.text(0.49, level, raw_text, color=color, fontsize=int(12*table_scale), horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "            ax.text(0.68, level, yield_text, color=color, fontsize=int(12*table_scale), horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "            ax.text(0.87, level, percentage, color=color, fontsize=int(12*table_scale), horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "        level -= 2*linespacing*table_scale\n",
    "\n",
    "    ax.set_title(title,pad=25,  fontsize= \"15\", color=\"#192655\")\n",
    "    for format in formats :\n",
    "        filename = name+'.'+format\n",
    "        full_name = path+filename\n",
    "        fig.savefig(full_name,dpi=240);\n",
    "        print(filename, \" saved at \", path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1ba490-b478-4a8d-9af6-23552f2f139a",
   "metadata": {},
   "source": [
    "**cuts_table** creates and saves a visual table of cut labels for an analysis.   \n",
    "**Input**: name (str, output file base name), title (str, plot title), labels (list of cut labels), formats (list of output file formats, e.g., ['png', 'pdf']), path (str, output directory).   \n",
    "**Output**: none (saves plot files).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "867c14d7-d252-4ff2-81fc-e4bb22cdeaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuts_table(name, title, labels, formats, path):\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    ax.text(0.25, 1.02, 'FCC Analyses: FCC-ee Simulation (Delphes)', fontsize=10, horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n",
    "    ax.text(0.92, 1.02, '$\\\\sqrt{s} = '+str(process['energy'])+' GeV$', fontsize=10, horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "    level, linespacing = 0.90, 0.05\n",
    "    ax.text(0.02, level, 'Cut Order', weight='bold', fontsize=12, horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "    ax.text(0.30, level, 'Label', weight='bold', fontsize=12, horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        level -= linespacing\n",
    "        ax.text(0.02, level, str(i), fontsize=10,horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "        ax.text(0.30, level, labels[i],fontsize=12, horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "    level -= 2*linespacing\n",
    "\n",
    "    ax.set_title(title,pad=25,  fontsize= \"15\", color=\"#192655\")\n",
    "    for format in formats :\n",
    "        filename = name+'.'+format\n",
    "        full_name = path+filename\n",
    "        fig.savefig(full_name,dpi=240);\n",
    "        print(filename, \" saved at \", path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa73364-ff05-4dcb-8548-afbf285c8035",
   "metadata": {},
   "source": [
    "**makeplot** creates a single kinematic histogram on a given Matplotlib axis.   \n",
    "**Input**: fig, ax (Matplotlib figure and axis objects), hist (data to plot), name (str, plot name), title (str, plot title), label (str, dataset label), xlabel, ylabel (str, axis labels), bins (int), xmin, xmax (plot range), log (bool, log scale on y-axis), stack (bool, stack histograms), color (str or color), histtype (str, e.g., 'step'), xticks (int, optional), cutflow_mode (bool, optional).   \n",
    "**Output**: none (plots directly).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5399559-1e4e-4fd0-8bc0-a1fee1880add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeplot(fig, ax, hist, name, title, label, xlabel, ylabel, bins, xmin, xmax, log, stack, color, histtype, xticks=10, cutflow_mode=False):\n",
    "    if len(hist) != 0 :\n",
    "        hep.histplot(\n",
    "            hist,\n",
    "            yerr=0,\n",
    "            histtype=histtype,\n",
    "            label=label,\n",
    "            color=color,\n",
    "            alpha=0.8,\n",
    "            stack=stack,\n",
    "            edgecolor='black',\n",
    "            linewidth=1,\n",
    "            # sort='label',\n",
    "            ax=ax\n",
    "        )\n",
    "\n",
    "    ax.text(*globals().get(\"FCC_text_location\", (0.27, 1.02,)), 'FCC Analyses: FCC-ee Simulation (Delphes)', fontsize=9, horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n",
    "    ax.text(*globals().get(\"sqrt_s_text_location\", (0.92, 1.02)), f'$\\\\sqrt{{s}} = {process['energy']} GeV$', fontsize=9, horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "    if  cutflow_mode:\n",
    "        ax.set_ylabel(ylabel)\n",
    "    else:\n",
    "        per_bin = '/'+str((xmax-xmin)/bins)\n",
    "        ax.set_ylabel(ylabel+per_bin+' [GeV]')\n",
    "        plt.xlim([xmin,xmax])\n",
    "        plt.xticks(np.linspace(xmin,xmax,xticks+1))\n",
    "        ax.xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "    ax.set_xlabel(xlabel)\n",
    "\n",
    "    if log :\n",
    "        ax.set_yscale('log')\n",
    "        plt.tick_params(axis='y', which='minor')\n",
    "    else:\n",
    "        ax.yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "\n",
    "    ax.set_title(title,pad=25,  fontsize= \"15\", color=\"#192655\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f797069-2a63-462e-b1f7-b39998e1ba4c",
   "metadata": {},
   "source": [
    "**generate_plots** and **process_group** processes multiple datasets to create yield, cutflow, and kinematic plots for a set of selections.   \n",
    "**Input**: input_dict (dict of datasets with cutflows and histograms), req_hists (dict specifying datasets, types, and colors), req_plots (list of histogram names to plot), selections (list of selection names), stack (list of booleans for stacked/unstacked plotting), log (list of booleans for linear/log scale), formats (list of output file formats), path (str, output directory), plotprops (plot styling properties).   \n",
    "**Output**: none (saves plots and tables).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "629e8271-c2c5-4179-9533-0a78b636d6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_group(key, sel, group_type, req_hists, input_dict, process):\n",
    "    \"\"\"Process Signal or Background group and return label, datasets, color, hists, unscaled_hists.\"\"\"\n",
    "    print(f'-->Type: {group_type}')\n",
    "    label = key\n",
    "    datasets = req_hists[key]['datasets']\n",
    "    color = req_hists[key]['color']\n",
    "    hists, unscaled_hists = [], []\n",
    "    \n",
    "    for i in datasets:\n",
    "        print(\"aaa\")\n",
    "        print(input_dict[i])\n",
    "        cutflow_hist = input_dict[i][sel]['cutflow']['Cutflow']\n",
    "        print(\"bbb\")\n",
    "        print(cutflow_hist)\n",
    "        cut_labels = input_dict[i][sel]['cutflow']['Labels']\n",
    "        cutflow_values = cutflow_hist.values()\n",
    "        Raw_Events = cutflow_values[0]\n",
    "        print(f'-->RawEvents for {i}: {Raw_Events}')\n",
    "\n",
    "        xsec_scale_factor = get_xsec_scale(i, Raw_Events, process['intLumi'])\n",
    "        print(f'-->xsec_scale for {i} = {xsec_scale_factor}')\n",
    "\n",
    "        Hist = input_dict[i][sel]['histograms']\n",
    "        scaled_hist = { name: xsec_scale_factor * hist for name, hist in Hist.items() }\n",
    "        scaled_hist['Cutflow'] = xsec_scale_factor * cutflow_hist\n",
    "        hists.append(scaled_hist)\n",
    "\n",
    "        unscaled_hist = copy.deepcopy(Hist)\n",
    "        unscaled_hist['Cutflow'] = cutflow_hist\n",
    "        unscaled_hists.append(unscaled_hist)\n",
    "\n",
    "    return label, datasets, color, accumulate(hists), accumulate(unscaled_hists), cut_labels, cutflow_values\n",
    "\n",
    "def generate_plots(input_dict, req_hists, req_plots, selections, stack, log, formats, path, plotprops):\n",
    "    show_plots=[\n",
    "        'fourmuons_mass',\n",
    "        'Z_res_mass',\n",
    "        'Z_non_res_mass',\n",
    "        'missing_p',\n",
    "        'fourmuons_pmin',\n",
    "        'vis_e_woMuons',\n",
    "\n",
    "        # 'iso_least_isolated_muon',\n",
    "        # 'selectedmuons_p',\n",
    "        # 'cos_theta_miss',\n",
    "        # 'Cutflow',\n",
    "        # 'Cuts_table',\n",
    "        # 'Yield',\n",
    "    ]\n",
    "    selections = ['sel0','sel1','sel2','sel3','sel4','sel5','sel6'] ## list of selection stages used for cutflow analysis\n",
    "    for sel in selections:\n",
    "        print('_________________________________________________________________')\n",
    "        print('---------------------','Selection:', sel ,'---------------------')\n",
    "        plot_path_selection = path+sel+'/'\n",
    "        if not os.path.exists(plot_path_selection):\n",
    "            os.makedirs(plot_path_selection)\n",
    "\n",
    "        #Get hist array for different backgrounds\n",
    "        label_list, label_list_signal = [], []\n",
    "        dataset_list, dataset_list_signal = [], []\n",
    "        color_list, color_list_signal = [], []\n",
    "        hist_list, hist_list_signal = [], []\n",
    "        unscaled_hist_list, unscaled_hist_list_signal = [], []\n",
    "\n",
    "        cut_labels, cutflow_values = None, None\n",
    "        \n",
    "        for key in req_hists:\n",
    "            print('-------------------------------------------------------------------')\n",
    "            print(f\"Key: {key}            Sample:{req_hists[key]['datasets']} \")\n",
    "            print('-------------------------------------------------------------------')\n",
    "\n",
    "            group_type = req_hists[key]['type']\n",
    "            if group_type in ['Signal', 'Background']:\n",
    "                label, datasets, color, hists, unscaled_hists, cut_labels, cutflow_values = process_group(\n",
    "                    key, sel, group_type, req_hists, input_dict, process\n",
    "                )\n",
    "\n",
    "                if group_type == 'Signal':\n",
    "                    label_list_signal.append(label)\n",
    "                    dataset_list_signal.append(datasets)\n",
    "                    color_list_signal.append(color)\n",
    "                    hist_list_signal.append(hists)\n",
    "                    unscaled_hist_list_signal.append(unscaled_hists)\n",
    "                else:\n",
    "                    label_list.append(label)\n",
    "                    dataset_list.append(datasets)\n",
    "                    color_list.append(color)\n",
    "                    hist_list.append(hists)\n",
    "                    unscaled_hist_list.append(unscaled_hists)\n",
    "            else:\n",
    "                raise TypeError('Unrecognised type in req_hists')\n",
    "\n",
    "        # Make Cut table\n",
    "        print('---------------------------------------------------------------')\n",
    "        print('Cuts Table : Info about the cuts')\n",
    "        print('---------------------------------------------------------------')\n",
    "        cuts_table(\n",
    "            name='Cuts_table',\n",
    "            title=f'{sel} cuts',\n",
    "            labels=cut_labels,\n",
    "            formats=formats,\n",
    "            path=plot_path_selection\n",
    "        )\n",
    "        print('---------------------------------------------------------------')\n",
    "\n",
    "        #Make Yield Plots\n",
    "        print('---------------------------------------------------------------')\n",
    "        print('Yield : Unscaled  and Scaled')\n",
    "        print('---------------------------------------------------------------')\n",
    "\n",
    "        print(\"DEBUG Yield input - scaled keys:\")\n",
    "        for idx, h in enumerate(hist_list_signal+hist_list):\n",
    "            print(idx, h.keys())\n",
    "        \n",
    "        print(\"DEBUG Yield input - unscaled keys:\")\n",
    "        for idx, h in enumerate(unscaled_hist_list_signal+unscaled_hist_list):\n",
    "            print(idx, h.keys())\n",
    "\n",
    "        \n",
    "        yield_plot(\n",
    "            name='Yield',\n",
    "            title=f'{sel} Yield',\n",
    "            keys=req_hists.keys(),\n",
    "            scaled=hist_list_signal+hist_list,\n",
    "            unscaled=unscaled_hist_list_signal+unscaled_hist_list,\n",
    "            formats=formats,\n",
    "            path=plot_path_selection,\n",
    "            plot_width=12\n",
    "        )\n",
    "        print('---------------------------------------------------------------')\n",
    "\n",
    "        # Add cutflow to plot_props\n",
    "        xticks = np.arange(len(cutflow_values))\n",
    "        plotprops = plotprops.assign(Cutflow = ['Cutflow',sel+' Cutflow','Cut Order','Events',len(xticks)+1,xticks[0],xticks[-1]])\n",
    "\n",
    "        # Start plotting\n",
    "        for hist_name in req_plots+['Cutflow']:\n",
    "            hist = [hists[hist_name] for hists in hist_list]\n",
    "            n_bkgs = len(hist)\n",
    "            hist_signal = [hists[hist_name] for hists in hist_list_signal]\n",
    "            n_sig = len(hist_signal)\n",
    "            cutflow_mode=False\n",
    "            if hist_name =='Cutflow':\n",
    "                cutflow_mode=True\n",
    "\n",
    "            print(hist_name, ' : ', plotprops[hist_name].title)\n",
    "            print('---------------------------------------------------------------')\n",
    "            for log_mode in log :\n",
    "                for stack_mode in stack:\n",
    "                    fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "                    order = globals().get(\"custom_mc_order\", None)\n",
    "                    if not order is None:\n",
    "                        ordered_hist, ordered_label_list, ordered_color_list = hist_sorter(order, hist, label_list, color_list)\n",
    "                    else:\n",
    "                        ordered_hist, ordered_label_list, ordered_color_list = hist, label_list, color_list\n",
    "\n",
    "                    #Backgrounds\n",
    "                    makeplot(\n",
    "                        fig=fig,\n",
    "                        ax=ax,\n",
    "                        hist=ordered_hist,\n",
    "                        name=plotprops[hist_name].name,\n",
    "                        title=plotprops[hist_name].title,\n",
    "                        label=ordered_label_list,\n",
    "                        xlabel=plotprops[hist_name].xlabel,\n",
    "                        ylabel=plotprops[hist_name].ylabel,\n",
    "                        bins=plotprops[hist_name].bins,\n",
    "                        xmin=plotprops[hist_name].xmin,\n",
    "                        xmax=plotprops[hist_name].xmax,\n",
    "                        log=log_mode,\n",
    "                        stack=True, #Always stack backgrounds\n",
    "                        color=ordered_color_list,\n",
    "                        histtype='fill',\n",
    "                        cutflow_mode=cutflow_mode,\n",
    "                        xticks=8\n",
    "                    )\n",
    "                    #Signal\n",
    "                    stop_plotting_signal = False\n",
    "                    if stack_mode and n_bkgs != 0 and n_sig != 0:\n",
    "                        sigl_hist = [h+sum(hist) for h in hist_signal] #Manual stacking because independent stacking is not supported in mplhep\n",
    "                    elif n_sig != 0 :\n",
    "                        sigl_hist = hist_signal\n",
    "                    else:\n",
    "                        stop_plotting_signal = True\n",
    "\n",
    "                    if not stop_plotting_signal:\n",
    "\n",
    "                        hep.histplot(\n",
    "                            sigl_hist,\n",
    "                            color=color_list_signal,\n",
    "                            label=label_list_signal,\n",
    "                            histtype='step',\n",
    "                            stack=False, #overridden by stack_mode bool\n",
    "                            linewidth=1,\n",
    "                            ax=ax\n",
    "                        )\n",
    "                    fig.legend(prop={\"size\":10},loc= globals().get(\"legend_location\", (0.74,0.74)), reverse=globals().get(\"Reverse_legend_labels\", False) )\n",
    "\n",
    "                    if log_mode :\n",
    "                        log_mode_text = 'log'\n",
    "                    else :\n",
    "                        log_mode_text = 'linear'\n",
    "\n",
    "                    if stack_mode :\n",
    "                        stack_mode_text = 'stacked'\n",
    "                    else :\n",
    "                        stack_mode_text = 'unstacked'\n",
    "                    for format in formats :\n",
    "                        filename = plotprops[hist_name].name+'_'+log_mode_text+'_'+stack_mode_text+'.'+format\n",
    "                        full_name = plot_path_selection+filename\n",
    "                        fig.savefig(full_name,dpi=240);\n",
    "                        print(filename, \" saved at \", plot_path_selection)\n",
    "                    if hist_name in show_plots:\n",
    "                        plt.show()\n",
    "                    else:\n",
    "                        plt.close()\n",
    "            print('-------------------------------------------------------------------')\n",
    "        print('_____________________________________________________________________\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d99220",
   "metadata": {},
   "source": [
    "# **Processor**     \n",
    "\n",
    "Perform event selection for the _H → ZZ* → 4μ_ channel. \n",
    "Apply physics cuts, reconstruct Z boson candidates, calculate relevant kinematic variables, produce histograms, and generate cutflow tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a85a65b7-6f75-4e44-ad1c-802b324485b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fourleptons(processor.ProcessorABC):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def process(self,events):\n",
    "        \n",
    "        ## Create a Packed Selection to store boolean masks for events passing each cut.\n",
    "        ## This allows later combination of cuts and generation of cutflow tables.\n",
    "        cut = PackedSelection()\n",
    "\n",
    "        ## ----------------------------\n",
    "        ## MUON SELECTION\n",
    "        ## ----------------------------\n",
    "\n",
    "        ## Extract muon candidates from the reconstructed particle collection.\n",
    "        ## events.Muonidx0.index gives indices of muons in ReconstructedParticles.\n",
    "        Muons = events.ReconstructedParticles[events.Muonidx0.index]\n",
    "        ## Store the muon's index for later reference when matching/removing.\n",
    "        Muons[\"index\"] = events.Muonidx0.index\n",
    "        ## Apply a basic p > 2 GeV momentum cut on each muon.\n",
    "        sel_muon = Muons.p > 2.0\n",
    "        ## Mask out muons failing the p > 2 cut.\n",
    "        selected_muons_p = ak.mask(Muons, sel_muon)\n",
    "\n",
    "        ## ----------------------------\n",
    "        ## REQUIRE AT LEAST 4 MUONS\n",
    "        ## ----------------------------\n",
    "\n",
    "        ## Count non-None muons per event; require > 3 (4 or more).\n",
    "        at_least_4_muons = ak.num(ak.drop_none(selected_muons_p), axis=1) > 3\n",
    "        ## Mask out events failing the >3 muon requirement.\n",
    "        selected_muons = ak.mask(selected_muons_p, at_least_4_muons)\n",
    "        \n",
    "        ## ----------------------------\n",
    "        ## BUILD ON-SHELL Z CANDIDATE\n",
    "        ## ----------------------------\n",
    "\n",
    "        ## Use resonanceBuilder_mass to form lepton pairs and select the one closest to 91.2 GeV (Z boson mass).\n",
    "        Z = resonanceBuilder_mass(resonance_mass=91.2, use_MC_Kinematics=False, leptons=selected_muons)\n",
    "\n",
    "        ## Keep only the best Z candidate per event.\n",
    "        zll = ak.firsts(Z)\n",
    "        ## Extract the two muons (l1, l2) making up this on-shell Z.\n",
    "        l1 = ak.firsts(selected_muons[selected_muons.index == zll.l1_index])\n",
    "        l2 = ak.firsts(selected_muons[selected_muons.index == zll.l2_index])\n",
    "        \n",
    "        ## ----------------------------\n",
    "        ## BUILD OFF-SHELL Z CANDIDATE\n",
    "        ## ----------------------------\n",
    "\n",
    "        ## Remove the muons already used for the on-shell Z from the pool.\n",
    "        mask = create_mask(zll.l1_index, zll.l2_index, selected_muons.index)\n",
    "        rest_of_muons = selected_muons[mask]\n",
    "\n",
    "        ## From the remaining muons, choose the two with the highest momentum.\n",
    "        ## c_mask - make sure there are 3rd and 4th muons and they have opposite charge\n",
    "        m1, m2, c_mask = getTwoHighestPMuons(rest_of_muons)\n",
    "\n",
    "        ## Form the off-shell Z candidate from these two muons.\n",
    "        non_res_Z = m1 + m2\n",
    "        ## Compute the opening angle between these two muons.\n",
    "        non_res_Z_angle = m1.deltaangle(m2)\n",
    "\n",
    "        ## ----------------------------\n",
    "        ## COLLECT ALL FOUR MUONS\n",
    "        ## ----------------------------\n",
    "\n",
    "        ## Combine all four muons into a single awkward array: \n",
    "        ## - On-shell Z muons (l1, l2)\n",
    "        ## - Off-shell Z muons (m1, m2)\n",
    "        fourMuons_collected = ak.concatenate(\n",
    "            (\n",
    "                ak.mask(l1,c_mask)[:, np.newaxis],\n",
    "                ak.mask(l2,c_mask)[:, np.newaxis],\n",
    "                m1[:, np.newaxis],\n",
    "                m2[:, np.newaxis]\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        ## Keep only events that truly have all four muons after masking.\n",
    "        fourMuons_collected = ak.mask(fourMuons_collected, ak.num(fourMuons_collected, axis=1) > 3)\n",
    "        ## Combine both Z candidates into one 4-muon system (4-vector sum).\n",
    "        fourMuons = ak.mask(zll, c_mask) + non_res_Z\n",
    "        \n",
    "        ## Minimum momentum among the four muons — used for a cut.\n",
    "        fourMuons_pmin = ak.min(fourMuons_collected.p, axis=1)\n",
    "        #print('fourMuons_pmin: ', fourMuons_pmin.head())\n",
    "\n",
    "        ## ----------------------------\n",
    "        ## REST OF EVENT CONTENT\n",
    "        ## ----------------------------\n",
    "\n",
    "        ## All reconstructed particles in events with >3 muons.\n",
    "        chosen_reco_4_mu = ak.mask(events.ReconstructedParticles, at_least_4_muons)\n",
    "        ## Apply additional mask from Z candidate building.\n",
    "        chosen_reco = ak.mask(chosen_reco_4_mu,c_mask)\n",
    "        #print('chosen_reco: ', chosen_reco.head() )\n",
    "\n",
    "        ## Remove the four selected muons from the event.\n",
    "        rest_of_particles = remove(chosen_reco, fourMuons_collected)\n",
    "        ## Sum the 4-vectors of all non-muon particles (visible energy excluding muons).\n",
    "        all_others = sum_all(rest_of_particles)\n",
    "        #print('all_others: ', all_others.head())\n",
    "\n",
    "        ## Compute recoil 4-vector against the visible system (used for missing energy).\n",
    "        Emiss = recoilBuilder(sum_all(chosen_reco), ecm=ecm)\n",
    "        pmiss = Emiss.E ## magnitude of missing momentum\n",
    "        #print('pmiss: ', pmiss.head())\n",
    "\n",
    "        ## ----------------------------\n",
    "        ## ISOLATION VARIABLES\n",
    "        ## ----------------------------\n",
    "\n",
    "        ## Compute cone-based isolation for each muon in the event.\n",
    "        ## Here, max_dr=0.523599 rad ≈ 30 degrees.\n",
    "        fourMuons_iso = coneIsolation(fourMuons_collected, rest_of_particles, min_dr=0.0, max_dr=0.523599)\n",
    "        #print('fourMuons_iso: ', fourMuons_iso.head())\n",
    "        ## Take the largest isolation value among the four muons (worst isolation).\n",
    "        fourMuons_min_iso = ak.max(fourMuons_iso, axis=1)\n",
    "        #print('fourMuons_min_iso: ', fourMuons_min_iso.head())\n",
    "\n",
    "        ## ----------------------------\n",
    "        ## DEFINE CUTS\n",
    "        ## ----------------------------\n",
    "\n",
    "        E = events.ReconstructedParticles.E\n",
    "        selected_muons = ak.mask(selected_muons, c_mask)\n",
    "        zll = ak.mask(zll, c_mask)\n",
    "\n",
    "        ## Store cuts in PackedSelection.\n",
    "        # Define individual cuts\n",
    "        cut.add('No cut', ak.all(E > 0, axis=1))  ## baseline: event has valid energies\n",
    "        cut.add('cut1', fourMuons_pmin > 5)    ## all muons p > 5 GeV\n",
    "        cut.add('cut2', pmiss < 20)   ## low missing energy\n",
    "        cut.add('cut3', all_others.E > 95)   ## large visible E outside muons\n",
    "        cut.add('cut4', (non_res_Z.m < 65) & (non_res_Z.m > 10))   ## off-shell Z mass window\n",
    "        cut.add('cut5', (fourMuons.m < 130) & (fourMuons.m > 120))   ## wide Higgs mass window\n",
    "        cut.add('cut6', (fourMuons.m < 125.5) & (fourMuons.m > 124))   ## narrow Higgs mass window\n",
    "        cut.add('at_least_4_muons', at_least_4_muons)   ## at least 4 muons\n",
    "\n",
    "        ## ----------------------------\n",
    "        ## DEFINE SELECTION COMBINATIONS\n",
    "        ## ----------------------------\n",
    "\n",
    "        ## Selections: A collection of cuts (event selections)\n",
    "        selections = ['sel0','sel1','sel2','sel3','sel4','sel5','sel6'] ## list of selection stages used for cutflow analysis\n",
    "        sel = {}\n",
    "        sel[0] = ['No cut']\n",
    "        sel[1] = ['cut1']\n",
    "        sel[2] = ['cut1','cut2']\n",
    "        sel[3] = ['cut1','cut2','cut3']\n",
    "        sel[4] = ['cut1','cut2','cut3','cut4']\n",
    "        sel[5] = ['cut1','cut2','cut3','cut4','cut5']\n",
    "        sel[6] = ['cut6']\n",
    "\n",
    "        ## ----------------------------\n",
    "        ## CUTFLOW RESULTS\n",
    "        ## ----------------------------\n",
    "\n",
    "        ## For each selection combination, produce a yield histogram (cutflow table).\n",
    "        sel_ocl = {key:cut.cutflow(*val).yieldhist() for key,val in sel.items()}\n",
    "\n",
    "        ## ----------------------------\n",
    "        ## VARIABLES AFTER EACH SELECTION\n",
    "        ## ----------------------------\n",
    "\n",
    "        ## Apply the selection to the relevant variables\n",
    "        vars_sel = {}\n",
    "        for key,selections in sel.items():\n",
    "            vars_sel[key] = {\n",
    "                'selectedmuons_p':selected_muons.p[cut.all(*selections)],\n",
    "                'fourmuons_mass':fourMuons.m[cut.all(*selections)],\n",
    "                'fourmuons_pmin':fourMuons_pmin[cut.all(*selections)],\n",
    "                'Z_res_mass':zll.m[cut.all(*selections)],\n",
    "                'Z_non_res_mass':non_res_Z.m[cut.all(*selections)],\n",
    "                'vis_e_woMuons':all_others.E[cut.all(*selections)],\n",
    "                'iso_least_isolated_muon':fourMuons_min_iso[cut.all(*selections)],\n",
    "                'missing_p':pmiss[cut.all(*selections)],\n",
    "                'cos_theta_miss':abs(np.cos(Emiss.theta))[cut.all(*selections)],\n",
    "            }\n",
    "\n",
    "        ## ----------------------------\n",
    "        ## HISTOGRAMS & FINAL OUTPUT\n",
    "        ## ----------------------------\n",
    "\n",
    "\n",
    "        Output = processor.dict_accumulator({\n",
    "            f'sel{i}': processor.dict_accumulator({\n",
    "                'histograms': processor.dict_accumulator({\n",
    "                    name: get_1Dhist(name, var, flatten=True) for name, var in vars_sel[i].items()\n",
    "                }),\n",
    "                'cutflow': processor.dict_accumulator({\n",
    "                    'Onecut': sel_ocl[i][0],\n",
    "                    'Cutflow': sel_ocl[i][1],\n",
    "                    'Labels': sel_ocl[i][2]\n",
    "                })\n",
    "            })\n",
    "            for i in range(7)\n",
    "        })\n",
    "\n",
    "\n",
    "        # Output = {\n",
    "        #     f'sel{i}': {\n",
    "        #         'histograms': {name: get_1Dhist(name, var, flatten=True) for name, var in vars_sel[i].items()},\n",
    "        #         'cutflow': {\n",
    "        #             'Onecut': sel_ocl[i][0],\n",
    "        #             'Cutflow': sel_ocl[i][1],\n",
    "        #             'Labels': sel_ocl[i][2]\n",
    "        #         }\n",
    "        #     }\n",
    "        #     for i in range(7)\n",
    "        # }\n",
    "\n",
    "        \n",
    "        return Output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f9be5b-25bc-4391-b1b5-b5de6096386a",
   "metadata": {},
   "source": [
    "# **Runner**   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6795b0e6-fb24-4964-ba9b-58797c5da6a3",
   "metadata": {},
   "source": [
    "Create an instance of the Fourleptons event processor for analyzing four-lepton events   \n",
    "Load the chosen event schema   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8dac3aa-df21-4c44-9b8b-f752a570a6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_instance = Fourleptons()\n",
    "schema_import_string, schema, schema_name = get_schema(use_schema, schema_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d72ee0-3823-48de-ad95-1271df1d9204",
   "metadata": {},
   "source": [
    "Load YAML metadata for the given process, including file locations and event counts.   \n",
    "Select a fraction of the files from the YAML data and return them in a Dask-compatible fileset, with the given redirector for remote access.   \n",
    "Split the fileset into runner_chunks roughly equal parts for parallel processing.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6143c790-b13d-476b-a3d5-7df9e94ff298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to /cvmfs/fcc.cern.ch\n",
      "Loaded : /cvmfs/fcc.cern.ch/FCCDicts/yaml/FCCee/winter2023/IDEA/wzp6_ee_qqH_HZZ_llll_ecm240/merge.yaml\n",
      "Loaded : /cvmfs/fcc.cern.ch/FCCDicts/yaml/FCCee/winter2023/IDEA/wzp6_ee_nunuH_HZZ_ecm240/merge.yaml\n",
      "Loaded : /cvmfs/fcc.cern.ch/FCCDicts/yaml/FCCee/winter2023/IDEA/p8_ee_Zqq_ecm240/merge.yaml\n",
      "Loaded : /cvmfs/fcc.cern.ch/FCCDicts/yaml/FCCee/winter2023/IDEA/p8_ee_ZZ_ecm240/merge.yaml\n",
      "Loaded : /cvmfs/fcc.cern.ch/FCCDicts/yaml/FCCee/winter2023/IDEA/p8_ee_WW_ecm240/merge.yaml\n",
      "Loaded : /cvmfs/fcc.cern.ch/FCCDicts/yaml/FCCee/winter2023/IDEA/wzp6_ee_tautauH_HWW_ecm240/merge.yaml\n",
      "Loaded : /cvmfs/fcc.cern.ch/FCCDicts/yaml/FCCee/winter2023/IDEA/wzp6_ee_ccH_HWW_ecm240/merge.yaml\n",
      "Loaded : /cvmfs/fcc.cern.ch/FCCDicts/yaml/FCCee/winter2023/IDEA/wzp6_ee_bbH_HWW_ecm240/merge.yaml\n",
      "Loaded : /cvmfs/fcc.cern.ch/FCCDicts/yaml/FCCee/winter2023/IDEA/wzp6_ee_mumuH_HWW_ecm240/merge.yaml\n",
      "Loaded : /cvmfs/fcc.cern.ch/FCCDicts/yaml/FCCee/winter2023/IDEA/wzp6_ee_mumuH_Hcc_ecm240/merge.yaml\n",
      "Loaded : /cvmfs/fcc.cern.ch/FCCDicts/yaml/FCCee/winter2023/IDEA/wzp6_ee_mumuH_Hbb_ecm240/merge.yaml\n",
      "Loaded : /cvmfs/fcc.cern.ch/FCCDicts/yaml/FCCee/winter2023/IDEA/wzp6_ee_mumuH_Hgg_ecm240/merge.yaml\n",
      "Loaded : /cvmfs/fcc.cern.ch/FCCDicts/yaml/FCCee/winter2023/IDEA/wzp6_ee_mumuH_HZa_ecm240/merge.yaml\n",
      "_________Loading fileset__________\n",
      "----------------------------------\n",
      "----------wzp6_ee_qqH_HZZ_llll_ecm240---------\n",
      "----------------------------------\n",
      "Total available events = 1200000\n",
      "Fraction needed = 1e-09\n",
      "Needed events = 0.0012000000000000001\n",
      "Assigned events = 100000\n",
      "Number of files = 1\n",
      "Files:\n",
      "\t root://eospublic.cern.ch//eos/experiment/fcc/ee/generation/DelphesEvents/winter2023/IDEA/wzp6_ee_qqH_HZZ_llll_ecm240/events_005041503.root\n",
      "----------------------------------\n",
      "----------wzp6_ee_nunuH_HZZ_ecm240---------\n",
      "----------------------------------\n",
      "Total available events = 1200000\n",
      "Fraction needed = 1e-09\n",
      "Needed events = 0.0012000000000000001\n",
      "Assigned events = 100000\n",
      "Number of files = 1\n",
      "Files:\n",
      "\t root://eospublic.cern.ch//eos/experiment/fcc/ee/generation/DelphesEvents/winter2023/IDEA/wzp6_ee_nunuH_HZZ_ecm240/events_125829782.root\n",
      "----------------------------------\n",
      "----------p8_ee_Zqq_ecm240---------\n",
      "----------------------------------\n",
      "Total available events = 100559248\n",
      "Fraction needed = 1e-09\n",
      "Needed events = 0.100559248\n",
      "Assigned events = 100000\n",
      "Number of files = 1\n",
      "Files:\n",
      "\t root://eospublic.cern.ch//eos/experiment/fcc/ee/generation/DelphesEvents/winter2023/IDEA/p8_ee_Zqq_ecm240/events_005174489.root\n",
      "----------------------------------\n",
      "----------p8_ee_ZZ_ecm240---------\n",
      "----------------------------------\n",
      "Total available events = 56162093\n",
      "Fraction needed = 1e-09\n",
      "Needed events = 0.056162093\n",
      "Assigned events = 100000\n",
      "Number of files = 1\n",
      "Files:\n",
      "\t root://eospublic.cern.ch//eos/experiment/fcc/ee/generation/DelphesEvents/winter2023/IDEA/p8_ee_ZZ_ecm240/events_136205668.root\n",
      "----------------------------------\n",
      "----------p8_ee_WW_ecm240---------\n",
      "----------------------------------\n",
      "Total available events = 373375386\n",
      "Fraction needed = 1e-09\n",
      "Needed events = 0.37337538600000003\n",
      "Assigned events = 100000\n",
      "Number of files = 1\n",
      "Files:\n",
      "\t root://eospublic.cern.ch//eos/experiment/fcc/ee/generation/DelphesEvents/winter2023/IDEA/p8_ee_WW_ecm240/events_192501667.root\n",
      "----------------------------------\n",
      "----------wzp6_ee_tautauH_HWW_ecm240---------\n",
      "----------------------------------\n",
      "Total available events = 400000\n",
      "Fraction needed = 1e-09\n",
      "Needed events = 0.0004\n",
      "Assigned events = 100000\n",
      "Number of files = 1\n",
      "Files:\n",
      "\t root://eospublic.cern.ch//eos/experiment/fcc/ee/generation/DelphesEvents/winter2023/IDEA/wzp6_ee_tautauH_HWW_ecm240/events_158197636.root\n",
      "----------------------------------\n",
      "----------wzp6_ee_ccH_HWW_ecm240---------\n",
      "----------------------------------\n",
      "Total available events = 1200000\n",
      "Fraction needed = 1e-09\n",
      "Needed events = 0.0012000000000000001\n",
      "Assigned events = 100000\n",
      "Number of files = 1\n",
      "Files:\n",
      "\t root://eospublic.cern.ch//eos/experiment/fcc/ee/generation/DelphesEvents/winter2023/IDEA/wzp6_ee_ccH_HWW_ecm240/events_135308188.root\n",
      "----------------------------------\n",
      "----------wzp6_ee_bbH_HWW_ecm240---------\n",
      "----------------------------------\n",
      "Total available events = 1000000\n",
      "Fraction needed = 1e-09\n",
      "Needed events = 0.001\n",
      "Assigned events = 100000\n",
      "Number of files = 1\n",
      "Files:\n",
      "\t root://eospublic.cern.ch//eos/experiment/fcc/ee/generation/DelphesEvents/winter2023/IDEA/wzp6_ee_bbH_HWW_ecm240/events_021005740.root\n",
      "----------------------------------\n",
      "----------wzp6_ee_mumuH_HWW_ecm240---------\n",
      "----------------------------------\n",
      "Total available events = 400000\n",
      "Fraction needed = 1e-09\n",
      "Needed events = 0.0004\n",
      "Assigned events = 100000\n",
      "Number of files = 1\n",
      "Files:\n",
      "\t root://eospublic.cern.ch//eos/experiment/fcc/ee/generation/DelphesEvents/winter2023/IDEA/wzp6_ee_mumuH_HWW_ecm240/events_071220024.root\n",
      "----------------------------------\n",
      "----------wzp6_ee_mumuH_Hcc_ecm240---------\n",
      "----------------------------------\n",
      "Total available events = 400000\n",
      "Fraction needed = 1e-09\n",
      "Needed events = 0.0004\n",
      "Assigned events = 100000\n",
      "Number of files = 1\n",
      "Files:\n",
      "\t root://eospublic.cern.ch//eos/experiment/fcc/ee/generation/DelphesEvents/winter2023/IDEA/wzp6_ee_mumuH_Hcc_ecm240/events_189916518.root\n",
      "----------------------------------\n",
      "----------wzp6_ee_mumuH_Hbb_ecm240---------\n",
      "----------------------------------\n",
      "Total available events = 300000\n",
      "Fraction needed = 1e-09\n",
      "Needed events = 0.00030000000000000003\n",
      "Assigned events = 100000\n",
      "Number of files = 1\n",
      "Files:\n",
      "\t root://eospublic.cern.ch//eos/experiment/fcc/ee/generation/DelphesEvents/winter2023/IDEA/wzp6_ee_mumuH_Hbb_ecm240/events_159112833.root\n",
      "----------------------------------\n",
      "----------wzp6_ee_mumuH_Hgg_ecm240---------\n",
      "----------------------------------\n",
      "Total available events = 400000\n",
      "Fraction needed = 1e-09\n",
      "Needed events = 0.0004\n",
      "Assigned events = 100000\n",
      "Number of files = 1\n",
      "Files:\n",
      "\t root://eospublic.cern.ch//eos/experiment/fcc/ee/generation/DelphesEvents/winter2023/IDEA/wzp6_ee_mumuH_Hgg_ecm240/events_076993011.root\n",
      "----------------------------------\n",
      "----------wzp6_ee_mumuH_HZa_ecm240---------\n",
      "----------------------------------\n",
      "Total available events = 400000\n",
      "Fraction needed = 1e-09\n",
      "Needed events = 0.0004\n",
      "Assigned events = 100000\n",
      "Number of files = 1\n",
      "Files:\n",
      "\t root://eospublic.cern.ch//eos/experiment/fcc/ee/generation/DelphesEvents/winter2023/IDEA/wzp6_ee_mumuH_HZa_ecm240/events_190924913.root\n"
     ]
    }
   ],
   "source": [
    "raw_yaml = load_yaml_fileinfo(process)\n",
    "myfileset = get_fileset(raw_yaml, fractions, redirector='root://eospublic.cern.ch/')\n",
    "fileset = break_into_many(input_fileset=myfileset,n=runner_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a6bda5-ee1f-4cf5-8fb6-25d09add1c18",
   "metadata": {},
   "source": [
    "Prepare each chunk of files (fl) for analysis by reading events, batching them, skipping bad files, and aligning data structures.   \n",
    "Run preprocess on every chunk in fileset.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e104079b-74da-4672-8621-fb4775d8b9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 102.07 ms\n",
      "[########################################] | 100% Completed | 7.13 ss\n"
     ]
    }
   ],
   "source": [
    "dataset_runnable, dataset_updated = zip(*[preprocess(\n",
    "    fl,\n",
    "    align_clusters=False,\n",
    "    step_size=50_000,\n",
    "    files_per_batch=1,\n",
    "    skip_bad_files=True,\n",
    "    save_form=False,\n",
    ") for fl in fileset ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5d666c-f37b-4fac-9f62-7940aefba0a1",
   "metadata": {},
   "source": [
    "Run the Coffea processor locally with Dask, looping over dataset chunks: skip already-computed outputs, processe the rest, and save results as .coffea files in the output directory.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e831f303-2d1d-4dd0-929d-eb6ad8b542c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### running locally - og code \n",
    "\n",
    "# if not os.path.exists(output_path):\n",
    "#     os.makedirs(output_path)\n",
    "# print(\"Executing locally with dask ...\")\n",
    "# computed_chunks = os.listdir(output_path)\n",
    "# for i in range(len(dataset_runnable)):\n",
    "#     print('Chunk : ',i)\n",
    "#     output_filename = output_file+'.coffea'\n",
    "#     if output_filename in computed_chunks and runner_cachedchunks:\n",
    "#         print(output_filename, \" is already computed.\")\n",
    "#         continue\n",
    "#     to_compute = apply_to_fileset(\n",
    "#                 processor,\n",
    "#                 max_chunks(dataset_runnable[i], runner_maxchunks),\n",
    "#                 schemaclass=schema,\n",
    "#                 uproot_options={\"filter_name\": lambda x : \"PARAMETERS\" not in x}\n",
    "#     )\n",
    "#     computed = dask.compute(to_compute, num_workers=1) #dask.execute->processor_instance.process(events)\n",
    "#     (Out,) = computed\n",
    "#     if runner_chunks < 2:\n",
    "#         output_filename = output_file\n",
    "#     print(\"Saving the output to : \" , output_filename)\n",
    "#     util.save(output= Out, filename=os.path.join(output_path,output_filename))\n",
    "#     print(f\"File {output_filename} saved at {output_path}\")\n",
    "# print(\"Execution completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0087854-92bc-4817-a083-7169930dd2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Dask cluster: <Client: 'tls://192.168.202.30:8786' processes=0 threads=0, memory=0 B>\n",
      "Processing fileset: 0\n",
      "4leptons.coffea.coffea is already computed. Skipping.\n",
      "All filesets processed successfully.\n"
     ]
    }
   ],
   "source": [
    "USE_DASK = True\n",
    "treename = \"events\"\n",
    "\n",
    "def filter_branches(branch_name: str) -> bool:\n",
    "    forbidden = [\"PARAMETERS\", \"_intMap\", \"_floatMap\", \"_stringMap\"]\n",
    "    return not any(substr in branch_name for substr in forbidden)\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "if USE_DASK:\n",
    "    client = Client(\"tls://192.168.202.30:8786\")\n",
    "    print(\"Connected to Dask cluster:\", client)\n",
    "    executor = processor.DaskExecutor(client=client, status=True)\n",
    "else:\n",
    "    executor = processor.FuturesExecutor(workers=1)\n",
    "\n",
    "computed_filesets = os.listdir(output_path)\n",
    "\n",
    "runner = processor.Runner(\n",
    "        executor=executor,\n",
    "        schema=schema,\n",
    "        savemetrics=True,\n",
    "        chunksize=runner_maxchunks,\n",
    "    )\n",
    "\n",
    "for i, fs in enumerate(fileset):\n",
    "    print(f\"Processing fileset: {i}\")\n",
    "    output_filename = output_file + \".coffea\"\n",
    "    if output_filename in computed_filesets:\n",
    "        print(f\"{output_filename} is already computed. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    output = runner(\n",
    "        fileset=fs,\n",
    "        processor_instance=Fourleptons(),\n",
    "        treename=treename,\n",
    "        iteritems_options={\"filter_name\": filter_branches},\n",
    "    )\n",
    "        \n",
    "    util.save(output=output, filename=os.path.join(output_path, output_filename))\n",
    "    print(f\"Fileset {i} saved to {output_filename}\")\n",
    "\n",
    "print(\"All filesets processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d41bdc1-659e-46cd-92eb-f31971e1d3bc",
   "metadata": {},
   "source": [
    "# Plotter:   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5d68c3-36f0-4dd6-a826-4070cec0338e",
   "metadata": {},
   "source": [
    "Find .coffea files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bafeee8c-f466-4395-9606-844d06b56baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected coffea files:\n",
      "\tBatch/4leptons.coffea.coffea\n",
      "Choosing:\n",
      "\t4leptons.coffea\n"
     ]
    }
   ],
   "source": [
    "coffea_files = glob.glob(input_path+'*.coffea')\n",
    "print('Detected coffea files:')\n",
    "for file in coffea_files : print('\\t'+file)\n",
    "print(f'Choosing:\\n\\t{output_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adefa8dc-1d2c-4747-b33c-8398ecf5e56b",
   "metadata": {},
   "source": [
    "Find chunked coffea files and combine them. If there is only one chunk no need to join chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b80f4afe-99af-44da-9c41-34f1fe13b6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joining chunks:\n"
     ]
    }
   ],
   "source": [
    "output_file=\"4leptons.coffea.coffea\"\n",
    "chunked_coffea_files = glob.glob(input_path+output_file+'-chunk*.coffea')\n",
    "if len(chunked_coffea_files) != 1 :\n",
    "    print('Joining chunks:')\n",
    "    chunk_index_list = []\n",
    "    chunk_list = []\n",
    "    for file in chunked_coffea_files:\n",
    "        print('\\t'+file)\n",
    "        chunk_list.append(file)\n",
    "        chunk_index_list.append(int(re.search('-chunk(.*).coffea',file).group(1)))\n",
    "    chunk_index_list.sort()\n",
    "\n",
    "    #Check if there are missing chunks\n",
    "    full_set = set(range(len(chunk_index_list)))\n",
    "    lst_set = set(chunk_index_list)\n",
    "    missing = list(full_set - lst_set)\n",
    "    if len(missing) != 0:\n",
    "        raise FileNotFoundError(f'Missing chunk indexes : {missing}')\n",
    "\n",
    "    #Load and accumulate all the chunks\n",
    "    input_list = [load(file) for file in chunk_list]\n",
    "    input = accumulate(input_list)\n",
    "\n",
    "else :\n",
    "    input = load(input_path + output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735c12d8-e879-49fa-84de-d4cb086b9be9",
   "metadata": {},
   "source": [
    "Plot the histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11374a60-48cb-4ef8-95d9-0ed6b27009b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "--------------------- Selection: sel0 ---------------------\n",
      "-------------------------------------------------------------------\n",
      "Key: qqH_HZZ            Sample:['wzp6_ee_qqH_HZZ_llll_ecm240'] \n",
      "-------------------------------------------------------------------\n",
      "-->Type: Signal\n",
      "aaa\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'wzp6_ee_qqH_HZZ_llll_ecm240'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m selections \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msel0\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msel1\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msel2\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msel3\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msel4\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msel5\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msel6\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m## list of selection stages used for cutflow analysis   \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# print(input['wzp6_ee_qqH_HZZ_llll_ecm240'].keys())\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mgenerate_plots\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq_hists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq_plots\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_props\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 74\u001b[0m, in \u001b[0;36mgenerate_plots\u001b[0;34m(input_dict, req_hists, req_plots, selections, stack, log, formats, path, plotprops)\u001b[0m\n\u001b[1;32m     72\u001b[0m group_type \u001b[38;5;241m=\u001b[39m req_hists[key][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSignal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBackground\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m---> 74\u001b[0m     label, datasets, color, hists, unscaled_hists, cut_labels, cutflow_values \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq_hists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m group_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSignal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     79\u001b[0m         label_list_signal\u001b[38;5;241m.\u001b[39mappend(label)\n",
      "Cell \u001b[0;32mIn[42], line 11\u001b[0m, in \u001b[0;36mprocess_group\u001b[0;34m(key, sel, group_type, req_hists, input_dict, process)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maaa\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43minput_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     12\u001b[0m     cutflow_hist \u001b[38;5;241m=\u001b[39m input_dict[i][sel][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcutflow\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCutflow\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'wzp6_ee_qqH_HZZ_llll_ecm240'"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(plot_path):\n",
    "    os.makedirs(plot_path)\n",
    "\n",
    "selections = ['sel0','sel1','sel2','sel3','sel4','sel5','sel6'] ## list of selection stages used for cutflow analysis   \n",
    "# print(input['wzp6_ee_qqH_HZZ_llll_ecm240'].keys())\n",
    "generate_plots(input, req_hists, req_plots, selections, stack, log, formats, plot_path, plot_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcd22c3-f78d-4dda-9813-f6d38f00292e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
